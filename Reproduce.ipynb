{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-19T18:47:27.683819Z",
     "start_time": "2025-01-19T18:47:24.461982Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774e5655047c171d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Defining the Adjacency Matrix (simetrically normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6aabe02a4dbb81",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-19T19:42:10.648119Z",
     "start_time": "2025-01-19T19:42:10.639696Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Device name: NVIDIA A100-SXM4-40GB\n"
     ]
    }
   ],
   "source": [
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Device name:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "347d8ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def normalize_adjacency_dense_gpu(A):\n",
    "    \"\"\"\n",
    "    Normalize adjacency matrix on GPU.\n",
    "    A: Dense adjacency matrix (torch.Tensor).\n",
    "    \"\"\"\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    A = A.to(device)  # Move to GPU if available\n",
    "    \n",
    "    # Ensure self-loops\n",
    "    #A = A + torch.eye(A.size(0), device=A.device)\n",
    "    \n",
    "    # Degree vector\n",
    "    row_sum = torch.sum(A, dim=1)\n",
    "    print(row_sum[0])\n",
    "    \n",
    "    # Avoid division by zero by adding a small epsilon\n",
    "    D_inv_sqrt = torch.diag(1.0 / torch.sqrt(1e-10+ row_sum ))\n",
    "    print(D_inv_sqrt[0])\n",
    "    # Normalize adjacency\n",
    "    normalized_A = D_inv_sqrt @ A @ D_inv_sqrt\n",
    "    \n",
    "    # Enforce symmetry (optional but helps to handle numerical instability)\n",
    "    normalized_A = (normalized_A + normalized_A.T) / 2.0\n",
    "    \n",
    "    return normalized_A\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd15f164b0648153",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Define the GCN Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7674ef8ed978f90",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-19T18:47:28.346875Z",
     "start_time": "2025-01-19T18:47:28.339402Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class GCNLayer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(GCNLayer, self).__init__()\n",
    "        self.weight = nn.Parameter(torch.randn(input_dim, output_dim))\n",
    "        #self.weight = nn.Parameter(torch.randn(output_dim, input_dim))  # output_dim should be first\n",
    "\n",
    "\n",
    "    def forward(self, X, A_tilde):\n",
    "        return A_tilde @ X @ self.weight\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49006fa248be27a7",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Inference Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c09f77a93ea6ad03",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-19T18:47:32.193578Z",
     "start_time": "2025-01-19T18:47:32.188017Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class InferenceModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
    "        super(InferenceModel, self).__init__()\n",
    "        self.gcn1 = GCNLayer(input_dim, hidden_dim)\n",
    "        self.gcn2_mu = GCNLayer(hidden_dim, latent_dim)\n",
    "        self.gcn2_logsigma = GCNLayer(hidden_dim, latent_dim)\n",
    "\n",
    "    def forward(self, X, A_tilde):\n",
    "        H = F.relu(self.gcn1(X, A_tilde))  # Shared first layer\n",
    "        mu = self.gcn2_mu(H, A_tilde)  # Mean matrix\n",
    "        log_sigma = self.gcn2_logsigma(H, A_tilde)  # Log-variance matrix\n",
    "        return mu, log_sigma\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef56856a0b72bb45",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Variational Grpah Auto-Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff3b3bbf607e4457",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-19T18:47:33.185231Z",
     "start_time": "2025-01-19T18:47:33.179795Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class VGAE(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
    "        super(VGAE, self).__init__()\n",
    "        self.encoder = InferenceModel(input_dim, hidden_dim, latent_dim)\n",
    "\n",
    "    def forward(self, X, A_tilde):\n",
    "        mu, log_sigma = self.encoder(X, A_tilde)\n",
    "        # Reparameterization trick\n",
    "        std = torch.exp(0.5 * log_sigma)\n",
    "        std = torch.clamp(std, max=1e5) \n",
    "        eps = torch.randn_like(std)\n",
    "        Z = mu + eps * std\n",
    "        Z = F.normalize(Z, p=2, dim=1)  # Normalize rows of Z to unit length\n",
    "\n",
    "        A_reconstructed = torch.sigmoid(torch.matmul(Z, Z.T))\n",
    "        return Z, A_reconstructed, mu, log_sigma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8907c838",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class VGAE_MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
    "        super().__init__()\n",
    "        self.encoder = InferenceModel(input_dim, hidden_dim, latent_dim)\n",
    "        \n",
    "        # MLP Decoder (2-layer perceptron)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(2 * latent_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim // 2, hidden_dim // 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim // 4, 1)\n",
    "            )\n",
    "        \n",
    "\n",
    "    def forward(self, X, A_tilde, edge_index=None):\n",
    "        mu, log_sigma = self.encoder(X, A_tilde)\n",
    "        \n",
    "        # Reparameterization trick\n",
    "        std = torch.exp(0.5 * log_sigma)\n",
    "        std = torch.clamp(std, max=1e5) \n",
    "        eps = torch.randn_like(std)\n",
    "        Z = mu + eps * std\n",
    "        Z = F.normalize(Z, p=2, dim=1)  # Normalize rows of Z to unit length\n",
    "        \n",
    "        batch_size = 10000  # Adjust based on memory\n",
    "        num_edges = edge_index.shape[1]\n",
    "        A_reconstructed_list = []\n",
    "\n",
    "        for i in range(0, num_edges, batch_size):\n",
    "            batch_edges = edge_index[:, i : i + batch_size]\n",
    "            src, dst = batch_edges\n",
    "            Z_concat = torch.cat([Z[src], Z[dst]], dim=1)\n",
    "            A_reconstructed_list.append(torch.sigmoid(self.decoder(Z_concat)).squeeze())\n",
    "\n",
    "        A_reconstructed = torch.cat(A_reconstructed_list)\n",
    "        print(A_reconstructed.shape)\n",
    "        return Z, A_reconstructed, mu, log_sigma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ad315de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedInnerProductDecoder(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(latent_dim))  # Learnable weight vector\n",
    "\n",
    "    def forward(self, Z):\n",
    "        Z_weighted = Z * self.weight  # Apply element-wise weight\n",
    "        A_reconstructed = torch.sigmoid(torch.matmul(Z, Z_weighted.T))  # Full adjacency matrix\n",
    "        return A_reconstructed\n",
    "\n",
    "\n",
    "class VGAE_W(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
    "        super().__init__()\n",
    "        self.encoder = InferenceModel(input_dim, hidden_dim, latent_dim)\n",
    "        self.decoder = WeightedInnerProductDecoder(latent_dim)\n",
    "\n",
    "    def forward(self, X, A_tilde):\n",
    "        mu, log_sigma = self.encoder(X, A_tilde)\n",
    "        \n",
    "        # Reparameterization trick\n",
    "        std = torch.exp(0.5 * log_sigma)\n",
    "        std = torch.clamp(std, max=1e5)\n",
    "        eps = torch.randn_like(std)\n",
    "        Z = mu + eps * std\n",
    "        Z = F.normalize(Z, p=2, dim=1)  # Normalize rows of Z to unit length\n",
    "\n",
    "        A_reconstructed = self.decoder(Z)  # No need for edge_index now\n",
    "\n",
    "        return Z, A_reconstructed, mu, log_sigma\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ab8fce30d3c266",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb389713323ee3d1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-19T18:47:34.115960Z",
     "start_time": "2025-01-19T18:47:34.109603Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def loss_function(A, A_reconstructed, mu, log_sigma):\n",
    "    # Reconstruction loss (Binary Cross-Entropy)\n",
    "    recon_loss = F.binary_cross_entropy(A_reconstructed, A, reduction='sum')\n",
    "\n",
    "    # KL Divergence\n",
    "    kl_loss = -0.5 * torch.sum(1 + log_sigma - mu.pow(2) - log_sigma.clamp(max=10).exp())\n",
    "\n",
    "    return recon_loss + kl_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4040da93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function_mlp(A, A_reconstructed, mu, log_sigma, edge_index):\n",
    "    src, dst = edge_index  # Edge indices\n",
    "\n",
    "    # If A_reconstructed is 1D, select indices correctly\n",
    "    A_pred = A_reconstructed[torch.arange(edge_index.shape[1])]\n",
    "\n",
    "    # Get true adjacency values\n",
    "    A_true = A[src, dst]\n",
    "\n",
    "    # BCE loss\n",
    "    recon_loss = F.binary_cross_entropy(A_pred, A_true, reduction='sum')\n",
    "\n",
    "    # KL Divergence\n",
    "    kl_loss = -0.5 * torch.sum(1 + log_sigma - mu.pow(2) - log_sigma.clamp(max=10).exp())\n",
    "\n",
    "    return recon_loss + kl_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd79b6477dd33912",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae948bb",
   "metadata": {},
   "source": [
    "## VGAE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a7195b60518a90",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Cora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c11fa520fbc66c95",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-19T18:47:41.575185Z",
     "start_time": "2025-01-19T18:47:36.971270Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/scur2863/graphs/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: CoraFull():\n",
      "======================\n",
      "Number of graphs: 1\n",
      "Number of features: 8710\n",
      "Number of classes: 70\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.datasets import CoraFull\n",
    "\n",
    "cora_dataset = CoraFull(root='GraphDatasets/Cora')\n",
    "print(f'Dataset: {cora_dataset}:')\n",
    "print('======================')\n",
    "print(f'Number of graphs: {len(cora_dataset)}')\n",
    "print(f'Number of features: {cora_dataset.num_features}')\n",
    "print(f'Number of classes: {cora_dataset.num_classes}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c7c260c39e1acd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-19T18:47:42.529637Z",
     "start_time": "2025-01-19T18:47:42.048523Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjacency matrix (A): tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.]])\n",
      "Node feature matrix (X): tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Graph data\n",
    "data = cora_dataset[0]  \n",
    "X = data.x  # features matrix (N x D)\n",
    "edge_index = data.edge_index  # Edge list (2 x E)\n",
    "\n",
    "# Create the adjacency matrix (A)\n",
    "num_nodes = X.size(0)\n",
    "A = torch.zeros((num_nodes, num_nodes))\n",
    "\n",
    "# Convert the edge_index to an adjacency matrix\n",
    "row, col = edge_index\n",
    "A[row, col] = 1\n",
    "A[col, row] = 1  # Since the graph is undirected\n",
    "\n",
    "# Optionally, add self-loops (diagonal elements set to 1)\n",
    "A.fill_diagonal_(1)\n",
    "\n",
    "print(\"Adjacency matrix (A):\", A)\n",
    "print(\"Node feature matrix (X):\", X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "42133397845d82d2",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import networkx as nx\n",
    "from itertools import combinations\n",
    "import torch\n",
    "from torch_geometric.utils import to_networkx, from_networkx\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def remove_edges_and_sample_optimized(edge_index, num_nodes, test_size=0.1, val_size=0.05):\n",
    "    \"\"\"\n",
    "    Optimized version of edge removal and sampling non-edges.\n",
    "    \n",
    "    Parameters:\n",
    "    - edge_index: Tensor (2, E) containing edges in COO format.\n",
    "    - num_nodes: int, number of nodes in the graph.\n",
    "    - test_size: float, fraction of edges to use for the test set.\n",
    "    - val_size: float, fraction of edges to use for the validation set.\n",
    "\n",
    "    Returns:\n",
    "    - train_edge_index: Tensor containing edges for training.\n",
    "    - val_edges: List of validation edges.\n",
    "    - test_edges: List of test edges.\n",
    "    - val_non_edges: List of validation non-edges.\n",
    "    - test_non_edges: List of test non-edges.\n",
    "    \"\"\"\n",
    "    # Convert edge_index to a set of edges for faster lookup\n",
    "    edges = set(map(tuple, edge_index.t().tolist()))\n",
    "    \n",
    "    # Generate all possible node pairs (i, j) for non-edges\n",
    "    all_pairs = set(combinations(range(num_nodes), 2))\n",
    "    non_edges = list(all_pairs - edges)\n",
    "\n",
    "    # Split edges into validation and test sets\n",
    "    edges = list(edges)\n",
    "    train_edges, temp_edges = train_test_split(edges, test_size=test_size + val_size, random_state=42)\n",
    "    val_edges, test_edges = train_test_split(temp_edges, test_size=test_size / (test_size + val_size), random_state=42)\n",
    "\n",
    "    # Sample non-edges for validation and test sets\n",
    "    num_val_non_edges = len(val_edges)\n",
    "    num_test_non_edges = len(test_edges)\n",
    "\n",
    "    val_non_edges = random.sample(non_edges, num_val_non_edges)\n",
    "    test_non_edges = random.sample(non_edges, num_test_non_edges)\n",
    "    # Recreate the training graph without validation and test edges\n",
    "    train_graph = nx.Graph()\n",
    "    train_graph.add_edges_from(train_edges)\n",
    "    train_graph.add_nodes_from(range(num_nodes))  # Add isolated nodes\n",
    "    train_edge_index = torch.tensor(list(train_graph.edges)).t().contiguous()\n",
    "\n",
    "    # Recreate training edge_index\n",
    "    train_edge_index = torch.tensor(train_edges).t().contiguous()\n",
    "\n",
    "    return train_edge_index, val_edges, test_edges, val_non_edges, test_non_edges,train_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe2dd7709dd505aa",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-01-19T18:30:48.460166Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train edge index shape: torch.Size([2, 107815])\n",
      "Number of validation edges: 6342\n",
      "Number of test edges: 12685\n",
      "Number of validation non-edges: 6342\n",
      "Number of test non-edges: 12685\n"
     ]
    }
   ],
   "source": [
    "# Extract edge_index and number of nodes\n",
    "edge_index = data.edge_index  # (2, E)\n",
    "num_nodes = data.num_nodes\n",
    "\n",
    "# Split edges and sample non-edges\n",
    "train_edge_index, val_edges, test_edges, val_non_edges, test_non_edges, train_graph = remove_edges_and_sample_optimized(edge_index, num_nodes)\n",
    "\n",
    "print(\"Train edge index shape:\", train_edge_index.shape)\n",
    "print(\"Number of validation edges:\", len(val_edges))\n",
    "print(\"Number of test edges:\", len(test_edges))\n",
    "print(\"Number of validation non-edges:\", len(val_non_edges))\n",
    "print(\"Number of test non-edges:\", len(test_non_edges))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e9b5a1",
   "metadata": {},
   "source": [
    "#### Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2e8c80c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(19., device='cuda:0')\n",
      "tensor([0.2294, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch_geometric.utils import to_dense_adj\n",
    "\n",
    "# Extract train graph adjacency matrix\n",
    "num_n=train_edge_index.max().item() + 1\n",
    "train_adj_matrix = to_dense_adj(train_edge_index, max_num_nodes=num_n)[0]  # Convert to dense adjacency matrix\n",
    "train_adj_matrix = train_adj_matrix.to(torch.float32)  # Ensure float type for computations\n",
    "train_adj_matrix = to_dense_adj(train_edge_index, max_num_nodes=num_nodes)[0] \n",
    "# Convert to SciPy sparse matrix\n",
    "# Create the adjacency matrix from the edge list (train_edge_index)\n",
    "#train_adj_matrix = to_dense_adj(train_edge_index, max_num_nodes=num_nodes)[0]\n",
    "\n",
    "# Enforce symmetry (add the transpose to ensure both directions are captured)\n",
    "#train_adj_matrix = train_adj_matrix + train_adj_matrix.T\n",
    "\n",
    "# Ensure that the diagonal entries are 1 (self-loops)\n",
    "#train_adj_matrix.fill_diagonal_(1.0)\n",
    "#train_adj_matrix = train_adj_matrix.clamp(max=1)\n",
    "# Node features\n",
    "if data.x is not None:\n",
    "    X = data.x  # Use provided node features\n",
    "else:\n",
    "    X = torch.eye(num_nodes)  # Use identity matrix if featureless\n",
    "\n",
    "# Normalize adjacency for training graph\n",
    "A_tilde_train = normalize_adjacency_dense_gpu(train_adj_matrix)\n",
    "\n",
    "# Initialize model\n",
    "input_dim = X.shape[1]\n",
    "hidden_dim = 32\n",
    "latent_dim = 16\n",
    "model = VGAE(input_dim, hidden_dim, latent_dim)\n",
    "\n",
    "model = model.to('cuda')  # Move model to GPU if available\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d22b68",
   "metadata": {},
   "source": [
    "#### Weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6cf72fc1",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m     X \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39meye(num_nodes)  \u001b[38;5;66;03m# Use identity matrix if featureless\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Normalize adjacency for training graph\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m A_tilde_train \u001b[38;5;241m=\u001b[39m \u001b[43mnormalize_adjacency_dense_gpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_adj_matrix\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Initialize model\u001b[39;00m\n\u001b[1;32m     19\u001b[0m input_dim \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n",
      "Cell \u001b[0;32mIn[3], line 7\u001b[0m, in \u001b[0;36mnormalize_adjacency_dense_gpu\u001b[0;34m(A)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mNormalize adjacency matrix on GPU.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03mA: Dense adjacency matrix (torch.Tensor).\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      6\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 7\u001b[0m A \u001b[38;5;241m=\u001b[39m \u001b[43mA\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Move to GPU if available\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Ensure self-loops\u001b[39;00m\n\u001b[1;32m     10\u001b[0m A \u001b[38;5;241m=\u001b[39m A \u001b[38;5;241m+\u001b[39m torch\u001b[38;5;241m.\u001b[39meye(A\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), device\u001b[38;5;241m=\u001b[39mA\u001b[38;5;241m.\u001b[39mdevice)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch_geometric.utils import to_dense_adj\n",
    "\n",
    "# Extract train graph adjacency matrix\n",
    "num_n=train_edge_index.max().item() + 1\n",
    "train_adj_matrix = to_dense_adj(train_edge_index, max_num_nodes=num_n)[0]  # Convert to dense adjacency matrix\n",
    "train_adj_matrix = train_adj_matrix.to(torch.float32)  # Ensure float type for computations\n",
    "train_adj_matrix = to_dense_adj(train_edge_index, max_num_nodes=num_nodes)[0] \n",
    "# Convert to SciPy sparse matrix\n",
    "# Create the adjacency matrix from the edge list (train_edge_index)\n",
    "#train_adj_matrix = to_dense_adj(train_edge_index, max_num_nodes=num_nodes)[0]\n",
    "\n",
    "# Enforce symmetry (add the transpose to ensure both directions are captured)\n",
    "train_adj_matrix = train_adj_matrix + train_adj_matrix.T\n",
    "\n",
    "# Ensure that the diagonal entries are 1 (self-loops)\n",
    "train_adj_matrix.fill_diagonal_(1.0)\n",
    "train_adj_matrix = train_adj_matrix.clamp(max=1)\n",
    "# Node features\n",
    "if data.x is not None:\n",
    "    X = data.x  # Use provided node features\n",
    "else:\n",
    "    X = torch.eye(num_nodes)  # Use identity matrix if featureless\n",
    "\n",
    "# Normalize adjacency for training graph\n",
    "A_tilde_train = normalize_adjacency_dense_gpu(train_adj_matrix)\n",
    "\n",
    "# Initialize model\n",
    "input_dim = X.shape[1]\n",
    "hidden_dim = 32\n",
    "latent_dim = 16\n",
    "model = VGAE_W(input_dim, hidden_dim, latent_dim)\n",
    "\n",
    "model = model.to('cuda')  # Move model to GPU if available\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7924d148",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_edge = torch.tensor(val_edges).contiguous()\n",
    "val_adj_matrix = to_dense_adj(val_edge, max_num_nodes=val_edge.shape[0])[0]  # Convert to dense adjacency matrix\n",
    "val_adj_matrix = val_adj_matrix.to(torch.float32)  # Ensure float type for computations\n",
    "\n",
    "A_tilde_val = normalize_adjacency_dense_gpu(val_adj_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c0607350",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (6342x6342 and 19793x8710)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 28\u001b[0m\n\u001b[1;32m     26\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 28\u001b[0m     Z_val, A_reconstructed_val, mu_val, log_sigma_val \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mA_tilde_val\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m     log_sigma_val \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mclamp(log_sigma_val, \u001b[38;5;28mmin\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m18\u001b[39m, \u001b[38;5;28mmax\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m18\u001b[39m)\n\u001b[1;32m     30\u001b[0m     val_loss \u001b[38;5;241m=\u001b[39m loss_function(val_adj_matrix\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m), A_reconstructed_val\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m), mu_val, log_sigma_val)\n",
      "File \u001b[0;32m~/graphs/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/graphs/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[8], line 19\u001b[0m, in \u001b[0;36mVGAE_W.forward\u001b[0;34m(self, X, A_tilde)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, A_tilde):\n\u001b[0;32m---> 19\u001b[0m     mu, log_sigma \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mA_tilde\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;66;03m# Reparameterization trick\u001b[39;00m\n\u001b[1;32m     22\u001b[0m     std \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mexp(\u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m log_sigma)\n",
      "File \u001b[0;32m~/graphs/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/graphs/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[5], line 9\u001b[0m, in \u001b[0;36mInferenceModel.forward\u001b[0;34m(self, X, A_tilde)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, A_tilde):\n\u001b[0;32m----> 9\u001b[0m     H \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgcn1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mA_tilde\u001b[49m\u001b[43m)\u001b[49m)  \u001b[38;5;66;03m# Shared first layer\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     mu \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgcn2_mu(H, A_tilde)  \u001b[38;5;66;03m# Mean matrix\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     log_sigma \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgcn2_logsigma(H, A_tilde)  \u001b[38;5;66;03m# Log-variance matrix\u001b[39;00m\n",
      "File \u001b[0;32m~/graphs/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/graphs/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[4], line 9\u001b[0m, in \u001b[0;36mGCNLayer.forward\u001b[0;34m(self, X, A_tilde)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, A_tilde):\n\u001b[0;32m----> 9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mA_tilde\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m \u001b[38;5;241m@\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (6342x6342 and 19793x8710)"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "num_epochs = 200\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass on training set\n",
    "    Z, A_reconstructed, mu, log_sigma = model(X.to('cuda'), A_tilde_train.to('cuda'))\n",
    "    \n",
    "    # Clamp log_sigma\n",
    "    log_sigma = torch.clamp(log_sigma, min=-18, max=18)\n",
    "\n",
    "    # Compute training loss\n",
    "    train_loss = loss_function(train_adj_matrix.to('cuda'), A_reconstructed.to('cuda'), mu, log_sigma)\n",
    "    train_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Store train loss\n",
    "    train_losses.append(train_loss.item())\n",
    "\n",
    "    # Validation step\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        Z_val, A_reconstructed_val, mu_val, log_sigma_val = model(X.to('cuda'), A_tilde_val.to('cuda'))\n",
    "        log_sigma_val = torch.clamp(log_sigma_val, min=-18, max=18)\n",
    "        val_loss = loss_function(val_adj_matrix.to('cuda'), A_reconstructed_val.to('cuda'), mu_val, log_sigma_val)\n",
    "        val_losses.append(val_loss.item())\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Train Loss: {train_loss.item():.4f}, Val Loss: {val_loss.item():.4f}\")\n",
    "\n",
    "# Plot Training & Validation Loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, num_epochs + 1), train_losses, label=\"Train Loss\", color=\"blue\")\n",
    "plt.plot(range(1, num_epochs + 1), val_losses, label=\"Validation Loss\", color=\"red\", linestyle=\"--\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training and Validation Loss\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "52104dc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.3056686159876943e+25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [44,0,0], thread: [0,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [44,0,0], thread: [1,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [44,0,0], thread: [2,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [44,0,0], thread: [3,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [44,0,0], thread: [4,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [44,0,0], thread: [5,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [44,0,0], thread: [6,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [44,0,0], thread: [7,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [44,0,0], thread: [8,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [44,0,0], thread: [9,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [44,0,0], thread: [10,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [44,0,0], thread: [11,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [44,0,0], thread: [12,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [44,0,0], thread: [13,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [44,0,0], thread: [14,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [44,0,0], thread: [15,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [44,0,0], thread: [16,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [44,0,0], thread: [17,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [44,0,0], thread: [18,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [44,0,0], thread: [19,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [44,0,0], thread: [20,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [44,0,0], thread: [21,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [44,0,0], thread: [22,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [44,0,0], thread: [23,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [44,0,0], thread: [24,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [44,0,0], thread: [25,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [44,0,0], thread: [26,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [44,0,0], thread: [27,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [44,0,0], thread: [28,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [44,0,0], thread: [29,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [44,0,0], thread: [30,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [44,0,0], thread: [31,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [182,0,0], thread: [96,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [182,0,0], thread: [97,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [182,0,0], thread: [98,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [182,0,0], thread: [99,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [182,0,0], thread: [100,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [182,0,0], thread: [101,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [182,0,0], thread: [102,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [182,0,0], thread: [103,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [182,0,0], thread: [104,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [182,0,0], thread: [105,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [182,0,0], thread: [106,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [182,0,0], thread: [107,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [182,0,0], thread: [108,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [182,0,0], thread: [109,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [182,0,0], thread: [110,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [182,0,0], thread: [111,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [182,0,0], thread: [112,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [182,0,0], thread: [113,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [182,0,0], thread: [114,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [182,0,0], thread: [115,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [182,0,0], thread: [116,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [182,0,0], thread: [117,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [182,0,0], thread: [118,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [182,0,0], thread: [119,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [182,0,0], thread: [120,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [182,0,0], thread: [121,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [182,0,0], thread: [122,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [182,0,0], thread: [123,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [182,0,0], thread: [124,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [182,0,0], thread: [125,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [182,0,0], thread: [126,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [182,0,0], thread: [127,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Update parameters\u001b[39;00m\n\u001b[1;32m     17\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 200\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass\n",
    "    Z, A_reconstructed, mu, log_sigma = model(X.to('cuda'), A_tilde_train.to('cuda'))\n",
    "    \n",
    "    # Clamp log_sigma to prevent extreme values\n",
    "    log_sigma = torch.clamp(log_sigma, min=-18, max=18)\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = loss_function(train_adj_matrix.to('cuda'), A_reconstructed.to('cuda'), mu, log_sigma)\n",
    "    loss.backward()\n",
    "\n",
    "    # Update parameters\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b4290b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1914470662144.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [26,0,0], thread: [96,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [26,0,0], thread: [97,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [26,0,0], thread: [98,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [26,0,0], thread: [99,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [26,0,0], thread: [100,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [26,0,0], thread: [101,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [26,0,0], thread: [102,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [26,0,0], thread: [103,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [26,0,0], thread: [104,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [26,0,0], thread: [105,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [26,0,0], thread: [106,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [26,0,0], thread: [107,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [26,0,0], thread: [108,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [26,0,0], thread: [109,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [26,0,0], thread: [110,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [26,0,0], thread: [111,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [26,0,0], thread: [112,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [26,0,0], thread: [113,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [26,0,0], thread: [114,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [26,0,0], thread: [115,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [26,0,0], thread: [116,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [26,0,0], thread: [117,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [26,0,0], thread: [118,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [26,0,0], thread: [119,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [26,0,0], thread: [120,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [26,0,0], thread: [121,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [26,0,0], thread: [122,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [26,0,0], thread: [123,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [26,0,0], thread: [124,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [26,0,0], thread: [125,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [26,0,0], thread: [126,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:94: operator(): block: [26,0,0], thread: [127,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;66;03m# Store loss for plotting\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m     train_losses\u001b[38;5;241m.\u001b[39mappend(\u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Plot Training Loss\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "num_epochs = 150\n",
    "train_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass\n",
    "    Z, A_reconstructed, mu, log_sigma = model(X.to('cuda'), A_tilde_train.to('cuda'))\n",
    "    \n",
    "    # Clamp log_sigma to prevent extreme values\n",
    "    log_sigma = torch.clamp(log_sigma, min=-18, max=18)\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = loss_function(train_adj_matrix.to('cuda'), A_reconstructed.to('cuda'), mu, log_sigma)\n",
    "    loss.backward()\n",
    "\n",
    "    # Update parameters\n",
    "    optimizer.step()\n",
    "\n",
    "    # Store loss for plotting\n",
    "    train_losses.append(loss.item())\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Plot Training Loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, num_epochs + 1), train_losses, label=\"Train Loss\", color=\"blue\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss Over Epochs\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51d4edf",
   "metadata": {},
   "source": [
    "#### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "da1f61f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.utils import to_dense_adj\n",
    "\n",
    "# Assume edge_index, num_nodes, and remove_edges_and_sample_optimized are defined\n",
    "# Extract train graph adjacency matrix\n",
    "num_n=train_edge_index.max().item() + 1\n",
    "train_adj_matrix = to_dense_adj(train_edge_index, max_num_nodes=num_n)[0]  # Convert to dense adjacency matrix\n",
    "train_adj_matrix = train_adj_matrix.to(torch.float32)  # Ensure float type for computations\n",
    "train_adj_matrix = to_dense_adj(train_edge_index, max_num_nodes=num_nodes)[0] \n",
    "# Convert to SciPy sparse matrix\n",
    "# Create the adjacency matrix from the edge list (train_edge_index)\n",
    "#train_adj_matrix = to_dense_adj(train_edge_index, max_num_nodes=num_nodes)[0]\n",
    "\n",
    "# Enforce symmetry (add the transpose to ensure both directions are captured)\n",
    "train_adj_matrix = train_adj_matrix + train_adj_matrix.T\n",
    "\n",
    "# Ensure that the diagonal entries are 1 (self-loops)\n",
    "train_adj_matrix.fill_diagonal_(1.0)\n",
    "train_adj_matrix = train_adj_matrix.clamp(max=1)\n",
    "# Node features\n",
    "if data.x is not None:\n",
    "    X = data.x  # Use provided node features\n",
    "else:\n",
    "    X = torch.eye(num_nodes)  # Use identity matrix if featureless\n",
    "\n",
    "# Normalize adjacency for training graph\n",
    "A_tilde_train = normalize_adjacency_dense_gpu(train_adj_matrix)\n",
    "\n",
    "# Initialize model\n",
    "input_dim = X.shape[1]\n",
    "hidden_dim = 32\n",
    "latent_dim = 16\n",
    "model = VGAE_MLP(input_dim, hidden_dim, latent_dim)\n",
    "\n",
    "model = model.to('cuda')  # Move model to GPU if available\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "b891643b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'edge_index' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[85], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m Z, A_reconstructed, mu, log_sigma \u001b[38;5;241m=\u001b[39m model(X\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m), A_tilde_train\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m), edge_index\u001b[38;5;241m=\u001b[39m\u001b[43medge_index\u001b[49m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Clamp log_sigma to prevent extreme values\u001b[39;00m\n\u001b[1;32m     10\u001b[0m log_sigma \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mclamp(log_sigma, \u001b[38;5;28mmin\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m18\u001b[39m, \u001b[38;5;28mmax\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m18\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'edge_index' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "num_epochs = 200\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass\n",
    "    Z, A_reconstructed, mu, log_sigma = model(X.to('cuda'), A_tilde_train.to('cuda'), edge_index=edge_index)\n",
    "    \n",
    "    # Clamp log_sigma to prevent extreme values\n",
    "    log_sigma = torch.clamp(log_sigma, min=-18, max=18)\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = loss_function_mlp(train_adj_matrix.to('cuda'), A_reconstructed, mu, log_sigma, train_edge_index.to('cuda'))\n",
    "    loss.backward()\n",
    "\n",
    "    # Update parameters\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d56e73",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d6af5b8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch-local/scur2863.9726859/ipykernel_844684/3338621847.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_edges = torch.tensor(test_edges, dtype=torch.long)\n",
      "/scratch-local/scur2863.9726859/ipykernel_844684/3338621847.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_non_edges = torch.tensor(test_non_edges, dtype=torch.long)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Convert the reconstructed adjacency matrix to CPU if necessary\n",
    "A_reconstructed = A_reconstructed.detach().cpu()\n",
    "\n",
    "test_edges = torch.tensor(test_edges, dtype=torch.long)\n",
    "test_non_edges = torch.tensor(test_non_edges, dtype=torch.long)\n",
    "# Get the scores for test edges and test non-edges\n",
    "test_edge_scores = A_reconstructed[test_edges[:, 0], test_edges[:, 1]].numpy()\n",
    "test_non_edge_scores = A_reconstructed[test_non_edges[:, 0], test_non_edges[:, 1]].numpy()\n",
    "\n",
    "# Combine scores and create labels\n",
    "scores = np.concatenate([test_edge_scores, test_non_edge_scores])\n",
    "labels = np.concatenate([np.ones(len(test_edge_scores)), np.zeros(len(test_non_edge_scores))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6a5806d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Convert the reconstructed adjacency matrix to CPU if necessary\n",
    "A_reconstructed = A_reconstructed.detach().cpu()\n",
    "\n",
    "# Ensure test edges and non-edges are tensors\n",
    "test_edges = torch.tensor(test_edges, dtype=torch.long)\n",
    "test_non_edges = torch.tensor(test_non_edges, dtype=torch.long)\n",
    "\n",
    "# Handle different decoder outputs\n",
    "if A_reconstructed.dim() == 2:  \n",
    "    # If A_reconstructed is a full adjacency matrix\n",
    "    test_edge_scores = A_reconstructed[test_edges[:, 0], test_edges[:, 1]].numpy()\n",
    "    test_non_edge_scores = A_reconstructed[test_non_edges[:, 0], test_non_edges[:, 1]].numpy()\n",
    "else:\n",
    "    # If A_reconstructed is a 1D tensor (edge probabilities only)\n",
    "    test_edge_scores = A_reconstructed[:len(test_edges)].numpy()\n",
    "    test_non_edge_scores = A_reconstructed[len(test_edges):].numpy()\n",
    "\n",
    "# Combine scores and create labels\n",
    "scores = np.concatenate([test_edge_scores, test_non_edge_scores])\n",
    "labels = np.concatenate([np.ones(len(test_edge_scores)), np.zeros(len(test_non_edge_scores))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "506bc5cc0e958f4f",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC Score: 0.6355253342373627\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Assuming y_true contains actual edge labels (1 for edges, 0 for non-edges)\n",
    "# and y_score contains the predicted scores for each pair of nodes\n",
    "roc_auc = roc_auc_score(labels, scores)\n",
    "print(f\"ROC-AUC Score: {roc_auc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "70a1c196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision (AP): 0.6649\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import average_precision_score\n",
    "ap_score = average_precision_score(labels, scores)\n",
    "print(f\"Average Precision (AP): {ap_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbcad91",
   "metadata": {},
   "outputs": [],
   "source": [
    "0.5897 y 0.5980"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ff2f59",
   "metadata": {},
   "source": [
    "### Patent dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cfdbe733",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import save_npz, load_npz\n",
    "import numpy as np\n",
    "import torch\n",
    "from scipy.sparse import coo_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "A = load_npz(\"combined_adj_small.npz\")\n",
    "X = load_npz(\"combined_features_matrix.npz\")\n",
    "\n",
    "X = torch.tensor(X.toarray(), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f7bb7d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix A is symmetric.\n"
     ]
    }
   ],
   "source": [
    "# Check if the matrix is symmetric\n",
    "if (A != A.T).nnz == 0:  # If the number of non-zero elements in (A - A.T) is zero\n",
    "    print(\"Matrix A is symmetric.\")\n",
    "else:\n",
    "    print(\"Matrix A is not symmetric.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c0f48d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def split_edges_and_sample(A, num_samples=None, test_size=0.1, val_size=0.05, random_state=42):\n",
    "    \"\"\"\n",
    "    Efficiently splits edges and samples non-edges.\n",
    "    \n",
    "    Parameters:\n",
    "    - A: scipy.sparse.coo_matrix (adjacency matrix)\n",
    "    - num_samples: Number of non-edges to sample (adjust based on graph size)\n",
    "    - test_size: Proportion of edges/non-edges for testing\n",
    "    - val_size: Proportion of edges/non-edges for validation\n",
    "    - random_state: Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "    - train_edge_index: Edge list for training\n",
    "    - val_edges, test_edges: Validation and test edge lists\n",
    "    - val_non_edges, test_non_edges: Validation and test non-edges\n",
    "    - train_graph: Sparse matrix representing the training graph\n",
    "    \"\"\"\n",
    "    A_coo = coo_matrix(A)\n",
    "    edges = np.vstack((A_coo.row, A_coo.col)).T  # Extract edges\n",
    "    num_nodes = A.shape[0]\n",
    "    \n",
    "    # Convert edges to a set for fast lookup\n",
    "    existing_edges = set(map(tuple, edges))\n",
    "    num_samples=len(edges)*0.15\n",
    "    # Randomly sample non-edges\n",
    "    np.random.seed(random_state)\n",
    "    non_edges = set()\n",
    "    while len(non_edges) < num_samples:\n",
    "        i = np.random.randint(0, num_nodes)\n",
    "        j = np.random.randint(0, num_nodes)\n",
    "        if i != j and (i, j) not in existing_edges and (j, i) not in existing_edges:\n",
    "            non_edges.add((i, j))\n",
    "    #print(existing_edges)\n",
    "    non_edges = np.array(list(non_edges))\n",
    "    \n",
    "    # Split edges into train, validation, and test sets\n",
    "    train_edges, temp_edges = train_test_split(edges, test_size=(test_size + val_size), random_state=random_state)\n",
    "    \n",
    "    val_edges, test_edges = train_test_split(temp_edges, test_size=(test_size / (test_size + val_size)), random_state=random_state)\n",
    "    \n",
    "    # Split sampled non-edges into validation and test sets\n",
    "    val_non_edges, test_non_edges = train_test_split(non_edges, test_size=(test_size / (test_size + val_size)), random_state=random_state)\n",
    "    \n",
    "    # Create a training graph (without val/test edges)\n",
    "    train_graph = coo_matrix(\n",
    "        (np.ones(len(train_edges)), (train_edges[:, 0], train_edges[:, 1])),\n",
    "        shape=A.shape\n",
    "    )\n",
    "    train_graph = train_graph + train_graph.T  # Ensure symmetry\n",
    "    \n",
    "    # Convert to PyTorch tensors\n",
    "    train_edge_index = torch.tensor(train_edges.T, dtype=torch.long)\n",
    "    val_edges = torch.tensor(val_edges, dtype=torch.long)\n",
    "    test_edges = torch.tensor(test_edges, dtype=torch.long)\n",
    "    val_non_edges = torch.tensor(val_non_edges, dtype=torch.long)\n",
    "    test_non_edges = torch.tensor(test_non_edges, dtype=torch.long)\n",
    "\n",
    "    return train_edge_index, val_edges, test_edges, val_non_edges, test_non_edges, train_graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "90308834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train edge index shape: torch.Size([2, 44438])\n",
      "Number of validation edges: 2614\n",
      "Number of test edges: 5229\n",
      "Number of validation non-edges: 2614\n",
      "Number of test non-edges: 5229\n"
     ]
    }
   ],
   "source": [
    "#A = A + A.T  # Ensure symmetry for an undirected graph\n",
    "#A[A > 1] = 1  # Remove duplicate edges\n",
    "\n",
    "# Split edges and sample non-edges\n",
    "train_edge_index, val_edges, test_edges, val_non_edges, test_non_edges, train_graph= split_edges_and_sample(A)\n",
    "\n",
    "print(\"Train edge index shape:\", train_edge_index.shape)\n",
    "print(\"Number of validation edges:\", len(val_edges))\n",
    "print(\"Number of test edges:\", len(test_edges))\n",
    "print(\"Number of validation non-edges:\", len(val_non_edges))\n",
    "print(\"Number of test non-edges:\", len(test_non_edges))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c7dc394c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_adjacency_sparse_gpu(A):\n",
    "    \"\"\"\n",
    "    Normalize adjacency matrix on GPU using sparse matrices.\n",
    "    A: Sparse adjacency matrix (torch.sparse.FloatTensor).\n",
    "    \"\"\"\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    A = A.to(device)  # Move to GPU if available\n",
    "    \n",
    "    # Ensure self-loops (can be done with sparse matrices too)\n",
    "    #eye = torch.eye(A.size(0), device=A.device).to_sparse()\n",
    "    #A = A + eye\n",
    "    \n",
    "    # Degree vector (sparse sum)\n",
    "    row_sum = torch.sum(A, dim=1) # Sparse sum and convert to dense\n",
    "    \n",
    "    # Avoid division by zero by adding a small epsilon\n",
    "    D_inv_sqrt = torch.diag(1.0 / torch.sqrt(row_sum + 1e-10))\n",
    "\n",
    "    # Normalize adjacency\n",
    "    normalized_A = D_inv_sqrt @ A @ D_inv_sqrt\n",
    "    \n",
    "    # Enforce symmetry\n",
    "    normalized_A = (normalized_A + normalized_A.T) / 2.0\n",
    "    \n",
    "    return normalized_A\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10bfa28",
   "metadata": {},
   "source": [
    "#### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9588794c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/scur2863/graphs/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3., device='cuda:0')\n",
      "tensor([0.5774, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch_sparse\n",
    "from torch_geometric.utils import to_scipy_sparse_matrix\n",
    "from torch_geometric.utils import to_dense_adj\n",
    "\n",
    "# Assume edge_index, num_nodes, and remove_edges_and_sample_optimized are defined\n",
    "# Extract train graph adjacency matrix\n",
    "\n",
    "# Extract train graph adjacency matrix\n",
    "num_nodes =  max(train_edge_index[0].max(), train_edge_index[1].max()) + 1\n",
    "\n",
    "\n",
    "train_adj_matrix = to_dense_adj(train_edge_index, max_num_nodes=num_nodes)[0] \n",
    "# Convert to SciPy sparse matrix\n",
    "\n",
    "\n",
    "#train_adj_matrix = to_scipy_sparse_matrix(train_edge_index, num_nodes=num_nodes)\n",
    "# Ensure adjacency matrix is sparse on GPU\n",
    "#train_adj_matrix = train_adj_matrix.to_sparse().to(device)  # Move sparse matrix to GPU\n",
    "\n",
    "# Convert directly to a PyTorch sparse tensor\n",
    "#train_adj_matrix = torch.tensor(train_adj_matrix.toarray(), dtype=torch.float32) \n",
    "# Ensure adjacency matrix is sparse on GPU\n",
    "#train_adj_matrix = train_adj_matrix.to_sparse().to('cuda')  # Move sparse matrix to GPU\n",
    "#train_adj_matrix = torch.sparse_coo_tensor(indices, values, size=(num_nodes, num_nodes), dtype=torch.float32).to(device)\n",
    "\n",
    "# Normalize adjacency for training graph\n",
    "A_tilde_train = normalize_adjacency_dense_gpu(train_adj_matrix.to(torch.float32))  # Normalize\n",
    "\n",
    "#train_adj_matrix = train_adj_matrix.to('cuda')\n",
    "#A_tilde_train = normalize_adjacency_dense_gpu(train_adj_matrix)\n",
    "\n",
    "\n",
    "# Initialize model\n",
    "input_dim = X.shape[1]\n",
    "hidden_dim = 64\n",
    "latent_dim = 32\n",
    "model = VGAE_MLP(input_dim, hidden_dim, latent_dim)\n",
    "\n",
    "model = model.to('cuda')  # Move model to GPU if available\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1894b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_epochs = 200\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass\n",
    "    Z, A_reconstructed, mu, log_sigma = model(X.to('cuda'), A_tilde_train.to('cuda'), edge_index=train_edge_index)\n",
    "    \n",
    "    # Clamp log_sigma to prevent extreme values\n",
    "    log_sigma = torch.clamp(log_sigma, min=-18, max=18)\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = loss_function_mlp(train_adj_matrix.to('cuda'), A_reconstructed, mu, log_sigma, train_edge_index.to('cuda'))\n",
    "    loss.backward()\n",
    "\n",
    "    # Update parameters\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b3a26b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix A is on device: cuda:0\n",
      "A after adding self-loops: torch.Size([17859, 17859])\n",
      "Row sums: torch.Size([17859])\n",
      "D_inv_sqrt: torch.Size([17859, 17859])\n",
      "Normalized A: torch.Size([17859, 17859])\n",
      "Final normalized A: torch.Size([17859, 17859])\n"
     ]
    }
   ],
   "source": [
    "A_tilde_train = normalize_adjacency_dense_gpu(train_adj_matrix)\n",
    "A_tilde_train = A_tilde_train.to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d3fb777c",
   "metadata": {},
   "outputs": [],
   "source": [
    "D = torch.diag(A_tilde_train.sum(dim=1).clamp(min=1).pow(-0.5))\n",
    "A_tilde_train = D @ A_tilde_train @ D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "aaa3b6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "A_tilde_train = A_tilde_train / A_tilde_train.sum(dim=1, keepdim=True).clamp(min=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "83314192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is A_tilde_train symmetric? True\n"
     ]
    }
   ],
   "source": [
    "is_symmetric = torch.allclose(train_adj_matrix, train_adj_matrix.T, atol=1e-6)\n",
    "print(\"Is A_tilde_train symmetric?\", is_symmetric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "26421a9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is A_tilde_train symmetric? True\n"
     ]
    }
   ],
   "source": [
    "is_symmetric = torch.allclose(A_tilde_train, A_tilde_train.T, atol=1e-6)\n",
    "print(\"Is A_tilde_train symmetric?\", is_symmetric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7d88a5c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.)\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in train_adj_matrix[0]:\n",
    "    if i!=0:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "130361bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5000, device='cuda:0')\n",
      "tensor(0.0928, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for i in A_tilde_train[0]:\n",
    "    if i!=0:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "33a102ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: encoder.gcn1.weight | Weights: tensor([[-0.5165, -0.7178,  0.1181,  ..., -1.4715, -0.0287,  0.2721],\n",
      "        [ 0.4688,  1.2197, -2.1986,  ..., -0.7330,  2.5639, -2.0083],\n",
      "        [ 0.6753,  0.8676,  1.9209,  ..., -0.5977, -0.2254,  0.2115],\n",
      "        ...,\n",
      "        [ 0.6558, -0.4602, -0.8751,  ..., -0.9342, -1.1248, -1.3540],\n",
      "        [ 1.0623, -0.6393, -0.4553,  ...,  0.3842,  0.6187, -0.0309],\n",
      "        [-0.6848, -0.2271,  0.1306,  ...,  0.3308,  0.1022, -1.2303]],\n",
      "       device='cuda:0')\n",
      "Layer: encoder.gcn2_mu.weight | Weights: tensor([[-0.4331, -0.6110, -0.2166, -1.6117, -1.2528, -0.2021, -0.0374,  0.4680,\n",
      "          0.2090,  0.2320, -2.9715, -1.7069, -1.4590,  0.1886, -1.2897,  0.7148],\n",
      "        [-1.3360,  0.7544, -0.2892,  0.1659, -0.0346,  1.4108, -1.3753, -0.3699,\n",
      "          2.5905,  1.6400, -0.6388, -1.6709, -1.2031, -2.4951,  1.5671, -0.1564],\n",
      "        [ 0.6477,  2.0286, -0.7522,  0.2703,  2.2812, -1.1301,  0.1586, -1.0083,\n",
      "          2.0195,  0.6130, -0.5320, -0.3704, -0.5705, -0.3191,  1.5993,  0.3465],\n",
      "        [-0.7247, -0.3878,  0.4720, -0.6736,  0.2972, -1.7135,  1.0189, -0.9323,\n",
      "          0.6734,  1.1559,  0.3119,  0.1555,  0.2716, -1.4690,  2.3237, -0.8936],\n",
      "        [ 0.6351,  1.7042, -2.5979, -1.0485,  0.4580, -0.9036,  2.1654, -0.1852,\n",
      "          0.1158, -1.4193,  0.9682,  0.8586,  0.5354, -0.5705,  0.3305,  1.2831],\n",
      "        [-2.4743,  0.3112,  3.2825,  1.2807,  0.4834, -2.2389,  0.1996,  0.6187,\n",
      "          0.6032, -0.3114,  0.1723,  0.3243, -0.9053, -0.1675,  0.1917, -0.6376],\n",
      "        [ 0.5090, -0.2600, -0.8385,  1.3298,  0.1590,  0.1630,  0.2017, -0.2124,\n",
      "          0.7252, -0.6304, -0.6483,  0.5136, -0.7854,  0.6432,  1.0487, -0.3739],\n",
      "        [ 0.0069,  0.2423,  0.1424, -1.0780,  0.4311, -0.1977, -1.2904,  0.9970,\n",
      "         -1.5368, -0.3778, -1.8128,  0.1189, -0.1062, -0.6368,  1.2621,  0.2720],\n",
      "        [-0.4208,  0.6616, -1.3541, -0.7614, -1.0658,  1.8317, -2.2460,  0.0627,\n",
      "         -0.0855,  0.8974, -0.8362, -0.0543,  0.2952, -0.9006, -0.8814, -0.8537],\n",
      "        [-0.3541,  0.6684,  0.1488,  1.0619,  0.5071,  0.1905, -0.9523, -0.2162,\n",
      "         -0.7818, -0.8568,  0.6459,  0.4543,  0.3792,  0.2066,  0.6146, -0.8441],\n",
      "        [-0.2897,  1.0256, -0.8277, -0.5796,  0.9249, -0.6273, -0.1853,  0.4567,\n",
      "         -1.8576,  0.2227, -1.3125,  0.0958,  0.3095,  0.4717,  0.3147,  0.1646],\n",
      "        [-0.9877, -0.8591,  1.7100,  1.2048,  0.2489, -1.8383,  0.8930, -0.4767,\n",
      "         -1.8567,  1.5939, -0.6144, -0.3565, -0.9343, -0.0894,  0.3148,  0.4582],\n",
      "        [-0.1098, -0.2117,  1.1024,  1.8177, -0.5774, -0.2301, -0.3455,  0.5382,\n",
      "          2.0436,  0.0118, -1.0245,  0.0355, -1.7803, -1.5589, -0.6885,  0.0836],\n",
      "        [ 0.5375,  0.6057, -1.1392,  0.4211, -0.3197, -0.6140, -0.2444, -0.0512,\n",
      "          0.9207,  0.9828,  0.3292,  0.3139,  2.0496, -0.1461, -0.0215,  0.8184],\n",
      "        [ 0.4991, -0.4793, -0.7155, -0.1292, -0.6599,  0.8441, -0.6019, -0.4561,\n",
      "         -0.4761,  0.6603, -0.7884, -0.3182, -0.1616, -1.0346,  0.7937, -0.1931],\n",
      "        [-1.9485, -0.2639,  0.3861, -0.1515, -0.9369,  1.6031, -0.4285, -0.5055,\n",
      "         -0.2301,  0.5360,  0.4529, -0.5130,  1.1447,  1.2364, -0.1260, -0.4809],\n",
      "        [-0.6096, -0.3307,  1.7803, -0.4653,  1.1979, -0.2844, -0.8450, -0.8907,\n",
      "          0.1193,  0.4175,  1.7568,  0.5878, -1.1032,  0.1700,  0.5216, -0.0970],\n",
      "        [-0.6489, -0.4029, -0.1426, -0.1971,  0.3458,  0.9624, -1.4890,  0.9012,\n",
      "          1.6018, -0.2383,  1.3470,  0.5464,  0.9200, -1.2233,  0.5947,  0.6485],\n",
      "        [ 1.5599,  1.8098,  0.2893,  0.7714,  1.6024,  0.7896, -1.0172,  1.4226,\n",
      "         -0.0337, -0.1112, -0.7652, -1.2498,  0.6398,  0.4980,  0.2074,  0.4265],\n",
      "        [-0.5668, -0.6369,  0.9448, -0.4555,  1.2585, -0.6879, -0.5471, -0.5019,\n",
      "          0.1372, -0.7350, -1.4845,  1.0337,  0.8461, -1.1978,  0.0869,  0.8178],\n",
      "        [ 0.5261, -0.6035, -0.3244, -0.9485, -0.7319,  0.5017,  1.2281,  0.6831,\n",
      "          1.3627,  0.1646, -0.3527,  0.1588,  0.7688,  0.5335, -1.2338,  0.5762],\n",
      "        [ 0.1105, -1.3631, -0.0487,  1.8385, -0.2368, -1.2666,  3.6373,  0.3738,\n",
      "          1.3307,  1.1511, -0.1237,  0.2596, -1.4248,  0.6654, -0.0692, -0.8229],\n",
      "        [ 1.7118, -0.0871,  0.1417, -0.4366, -0.2077, -0.5753, -0.5271, -0.7983,\n",
      "          0.3228, -1.3321, -0.6970,  1.2389, -0.8566,  0.2663,  0.5961,  1.1115],\n",
      "        [ 0.6502, -2.1386, -1.2909,  1.5644, -2.5723,  0.3688, -0.9905,  0.2560,\n",
      "          0.8047, -1.8731, -1.1930,  0.0299,  1.7910, -0.8401, -1.3434, -0.4795],\n",
      "        [-0.0506, -1.3546,  0.9264, -0.1165, -0.3894,  1.5423, -0.5307, -0.0687,\n",
      "          0.8819, -1.0517, -1.3408, -0.3983, -1.3571, -1.2446,  1.0623,  0.2394],\n",
      "        [-1.4850, -0.9447, -0.1218,  0.9937,  0.7626, -1.6785, -1.3495,  1.4396,\n",
      "         -0.4503,  1.0410, -0.5838, -0.6377,  0.8155, -0.3192, -0.8561, -0.2353],\n",
      "        [-0.4140,  0.9017,  0.6961, -1.9164,  1.1740, -0.6646,  0.3825,  0.2635,\n",
      "         -0.5214, -1.4053, -0.0789, -0.3105,  1.9671, -0.6363, -1.2639,  0.3357],\n",
      "        [ 0.4758,  0.2243,  0.2342,  1.5846, -0.6617, -0.4718, -0.5260,  1.1889,\n",
      "         -1.0968, -0.3949, -0.0636, -0.2738,  0.1469, -0.4901, -0.3023, -0.4553],\n",
      "        [-0.1086, -0.4207, -0.5905, -1.1071, -0.0990,  0.6518,  0.3766,  0.9659,\n",
      "         -1.2407,  1.3184,  2.6264, -0.1443,  1.4958,  0.9515, -0.1447, -0.1473],\n",
      "        [-1.0797, -2.0757,  0.6546, -0.5570, -1.2056, -0.4646,  1.0164,  0.0189,\n",
      "         -1.9414, -0.8983,  0.0825, -1.8804,  0.7827, -0.3820, -0.3474, -0.2514],\n",
      "        [ 2.3245,  0.5392, -0.0985,  0.5198, -0.3104,  0.0313,  0.2577, -1.0381,\n",
      "          0.2218,  0.2697,  0.4702,  0.8612,  0.7376, -0.1742,  0.7463,  0.4270],\n",
      "        [ 0.1316, -0.3708,  1.3028,  0.9441,  0.4864,  1.6367, -1.0872, -0.5066,\n",
      "          0.9931, -0.6871, -1.2108,  0.9020,  0.9259, -1.1238, -1.2517, -1.2459]],\n",
      "       device='cuda:0')\n",
      "Layer: encoder.gcn2_logsigma.weight | Weights: tensor([[ 5.0416e-02, -1.5042e-01, -1.1217e+00,  6.0025e-01,  4.9756e-01,\n",
      "         -8.8602e-01, -2.7154e-01, -1.9980e-01, -2.5236e+00,  2.8690e-01,\n",
      "         -9.9674e-01,  6.5829e-02, -7.8100e-01, -9.7901e-01, -7.2545e-01,\n",
      "          1.0645e+00],\n",
      "        [ 1.1040e+00, -5.3484e-01,  1.1111e+00,  5.4198e-01,  6.1880e-01,\n",
      "          1.4233e+00, -1.0386e+00,  1.9942e+00,  1.7265e+00, -1.9401e+00,\n",
      "         -2.7874e-01, -1.1412e+00,  1.2214e+00,  1.0508e+00, -5.6715e-01,\n",
      "         -2.4320e+00],\n",
      "        [-1.6041e+00,  7.1605e-01, -9.5891e-01,  2.2843e-01,  7.4694e-01,\n",
      "          1.7988e+00, -7.5673e-01,  7.4271e-01,  7.0790e-01, -1.7319e+00,\n",
      "         -2.1192e-01, -2.2113e-01, -1.1682e+00, -7.2402e-01,  2.0795e+00,\n",
      "         -4.8080e-01],\n",
      "        [ 7.2576e-01, -9.1010e-01,  2.5735e-02,  1.1586e+00, -6.2186e-01,\n",
      "          1.0696e+00, -8.2803e-01, -1.5429e+00, -9.3472e-01,  4.5294e-01,\n",
      "          1.3950e+00, -6.6233e-01,  2.6457e-01,  9.5790e-01, -8.0919e-01,\n",
      "         -7.9557e-01],\n",
      "        [-1.3154e+00,  1.0024e+00,  2.1564e-01, -2.7204e-01,  3.0128e-01,\n",
      "         -5.2446e-01,  4.0648e-01,  1.5023e+00,  4.7118e-01, -1.0814e+00,\n",
      "          7.1520e-01, -6.7234e-01, -2.6962e-01,  3.1208e-01,  1.1087e+00,\n",
      "          4.8822e-01],\n",
      "        [-6.6915e-01,  1.9275e+00, -6.7388e-01, -1.2815e+00,  1.6864e+00,\n",
      "          5.9825e-01,  2.0577e+00,  4.9342e-01,  9.4452e-01,  1.1317e+00,\n",
      "          4.4171e-01, -6.2280e-01,  1.1254e+00,  7.6340e-02, -5.8930e-01,\n",
      "         -2.0411e-01],\n",
      "        [ 8.8321e-01, -1.5027e-01, -6.8215e-01, -1.4028e+00, -6.7926e-02,\n",
      "          1.0895e+00,  5.0632e-01,  3.4103e-01,  8.1597e-01,  1.0278e+00,\n",
      "         -8.2369e-01, -2.3924e-01,  2.0723e-01,  5.1024e-01,  4.4263e-01,\n",
      "         -4.5132e-01],\n",
      "        [-5.7525e-02, -3.1670e-01, -1.4771e+00,  1.6952e+00,  1.4893e+00,\n",
      "          9.4610e-01, -2.0720e-01, -8.2442e-01, -1.1154e+00,  9.8450e-01,\n",
      "         -2.6082e+00,  2.0417e-02, -7.7214e-02,  8.0614e-02,  2.5246e-01,\n",
      "          5.9206e-01],\n",
      "        [-2.6225e-01,  1.5286e+00,  1.1475e+00, -4.1143e-02,  1.4134e-01,\n",
      "          9.1456e-01, -6.6752e-02, -9.3882e-01,  1.8613e+00,  3.2782e-01,\n",
      "          8.9436e-01, -1.3821e-01,  5.9361e-01, -1.4901e+00,  9.6589e-02,\n",
      "          4.8680e-01],\n",
      "        [ 1.8367e+00,  6.1100e-02,  1.6864e+00,  4.9405e-01,  8.8968e-01,\n",
      "          3.9385e-01,  6.7651e-01,  3.0533e-01,  2.5694e-01,  1.1741e+00,\n",
      "          7.6975e-01, -6.8233e-01,  7.7239e-01,  5.7867e-01,  6.3165e-01,\n",
      "         -6.9635e-01],\n",
      "        [ 2.4304e-01, -1.8383e+00,  7.7574e-01, -1.1186e-01,  4.9216e-01,\n",
      "         -1.7336e+00, -2.1548e-01, -3.6477e-02,  9.3892e-01, -5.9169e-02,\n",
      "         -1.1961e-01, -5.7012e-01, -1.4930e-01,  1.7364e+00, -6.2574e-01,\n",
      "         -1.3995e+00],\n",
      "        [ 6.1337e-01,  5.5378e-01, -1.0162e-01, -1.6538e+00, -1.3386e+00,\n",
      "         -2.0911e+00, -1.4493e+00,  7.5718e-01, -1.0708e+00, -1.0615e-03,\n",
      "          4.6110e-01, -4.9930e-01,  1.3104e+00, -1.5218e+00,  1.1286e-01,\n",
      "         -9.5385e-01],\n",
      "        [ 1.0181e+00, -8.3537e-01,  8.8585e-02, -9.9927e-01,  2.1161e+00,\n",
      "          1.1335e+00, -1.0745e+00, -1.1695e+00, -2.0867e-01,  1.0191e+00,\n",
      "         -1.3948e+00, -6.6272e-02, -1.9550e+00, -4.8560e-01, -1.9144e-01,\n",
      "         -3.3035e-02],\n",
      "        [-1.4441e+00, -7.9525e-01,  3.7225e-01,  5.6054e-01, -9.8172e-01,\n",
      "         -1.5731e+00,  2.9250e+00,  5.8966e-01, -8.3257e-01,  5.4518e-01,\n",
      "         -8.7365e-01, -3.8222e-01, -5.1851e-01, -1.0626e+00,  1.3567e+00,\n",
      "          9.1959e-01],\n",
      "        [ 7.0352e-01,  1.4413e+00, -3.4855e-01, -2.5993e-01, -1.5523e+00,\n",
      "          1.1047e+00,  6.8075e-02,  1.0666e+00,  1.2026e+00,  3.6617e-01,\n",
      "          3.1623e-02,  3.5354e-01, -8.6490e-01,  1.5053e-01, -2.5966e-01,\n",
      "         -4.1012e-01],\n",
      "        [ 3.8834e-01,  3.1229e-01, -5.8705e-01,  2.6345e-01, -1.1587e-01,\n",
      "          2.5430e-01,  7.6034e-01, -7.7247e-01, -7.9690e-01, -3.3935e-01,\n",
      "         -7.8965e-01, -5.6324e-01,  1.6345e-01,  6.9759e-01, -7.1539e-02,\n",
      "          3.1273e-01],\n",
      "        [ 1.9390e-01,  5.9252e-02,  2.6380e-01,  6.7447e-02, -7.4973e-01,\n",
      "          5.4093e-01,  6.7701e-01,  1.4089e+00,  8.9536e-01, -9.6660e-01,\n",
      "          6.5721e-01,  1.8431e+00, -2.5408e-01, -6.8554e-01, -7.1862e-01,\n",
      "          5.9190e-02],\n",
      "        [-5.3202e-01, -3.1279e-01,  1.0173e+00,  1.0783e+00,  1.9642e+00,\n",
      "          4.3559e-02,  1.3110e+00,  5.5815e-01, -1.2332e+00, -1.2260e+00,\n",
      "         -3.8050e-01, -7.5844e-01,  8.4237e-02,  3.1201e+00, -8.5162e-01,\n",
      "         -9.8314e-01],\n",
      "        [-4.9398e-01,  5.3945e-01,  1.6463e+00,  1.1251e+00, -5.0930e-01,\n",
      "         -3.9078e-01, -9.6544e-01, -1.2567e-01, -9.3244e-01, -3.3533e-01,\n",
      "         -8.9601e-01,  1.5910e+00, -1.8984e-01,  1.3146e+00,  7.0851e-01,\n",
      "         -3.7964e-01],\n",
      "        [ 4.5821e-01,  1.9528e+00, -3.9138e-01,  5.0809e-01,  4.1671e-01,\n",
      "          9.3263e-01,  3.8342e-01,  9.9587e-01, -1.4085e+00,  1.0159e+00,\n",
      "         -1.2344e-02,  1.3459e-01,  7.8787e-01,  1.0186e+00,  1.2970e+00,\n",
      "         -3.8398e-02],\n",
      "        [ 8.8230e-02,  8.1142e-01, -5.5664e-01,  3.3903e-01,  1.3612e+00,\n",
      "         -1.4233e-03, -3.0226e-01, -3.8297e-01, -7.5151e-01, -9.3074e-01,\n",
      "         -5.3117e-01, -2.0312e+00, -9.3148e-01, -1.3775e-02,  1.1112e+00,\n",
      "          5.0997e-01],\n",
      "        [-4.6429e-01,  1.4718e-03, -6.5086e-01,  1.2365e+00,  6.6227e-01,\n",
      "          1.3993e+00,  1.5017e+00, -8.7194e-01,  2.5366e+00,  6.9082e-01,\n",
      "          1.3382e+00,  4.7311e-01,  1.0636e+00,  4.8176e-01,  3.9330e-01,\n",
      "         -8.7338e-01],\n",
      "        [-7.7525e-01, -8.1380e-01, -4.0800e-02, -4.2110e-01, -3.1268e-01,\n",
      "         -5.7283e-01, -1.7276e+00,  4.8870e-01, -1.9766e-01,  1.1014e+00,\n",
      "          1.5487e+00, -6.1326e-01, -3.3206e-01, -1.1907e-01, -1.9671e+00,\n",
      "         -9.1805e-02],\n",
      "        [ 1.6581e+00,  3.6562e-01,  3.7584e-01,  1.5442e+00,  5.9494e-01,\n",
      "         -2.0625e+00,  1.5563e-02, -2.4816e+00, -2.3085e+00, -1.1551e-01,\n",
      "          5.4912e-01,  1.1984e+00, -1.1056e+00, -1.3364e+00, -6.6012e-01,\n",
      "         -2.7218e-03],\n",
      "        [ 7.0940e-01,  1.3506e+00,  2.7510e-01,  9.8898e-01,  2.8981e-01,\n",
      "         -1.5234e+00,  1.6544e+00, -2.7060e-01, -1.0536e+00, -1.4015e+00,\n",
      "          8.1069e-01,  1.9375e-02, -4.5594e-01, -6.4039e-02, -2.0034e-01,\n",
      "          3.2844e-01],\n",
      "        [ 1.1392e+00,  9.2389e-01,  4.8893e-01, -9.5784e-01,  4.6732e-01,\n",
      "          1.5933e+00,  1.5535e+00, -1.9822e-02,  1.4869e-01,  9.8234e-01,\n",
      "          9.0651e-01, -6.5497e-02, -1.7347e-01,  1.3353e+00, -6.4482e-01,\n",
      "         -5.3095e-01],\n",
      "        [ 9.0246e-01, -2.1842e-01, -4.5821e-01, -1.0901e+00,  7.8414e-01,\n",
      "          2.2112e-02, -1.9411e-01, -8.5106e-01,  9.6572e-02,  3.6523e-01,\n",
      "          6.3075e-01,  2.1584e+00,  1.8712e-01,  2.4405e-01,  1.8928e+00,\n",
      "         -6.9199e-02],\n",
      "        [-1.7640e+00, -1.2028e+00, -5.2862e-02, -1.1940e+00, -4.5491e-02,\n",
      "         -7.0453e-01, -7.1217e-01, -9.3702e-02, -2.4657e-01, -1.5359e+00,\n",
      "         -1.1761e+00, -3.6771e-01, -1.3920e+00, -1.7870e+00,  3.3736e-01,\n",
      "         -3.3737e-02],\n",
      "        [ 1.6545e-01, -2.5712e-01,  1.5257e+00, -9.2167e-01, -1.8262e-01,\n",
      "         -3.7347e-01,  7.3900e-02,  2.9600e-02, -8.3350e-01,  8.7804e-01,\n",
      "         -1.8821e+00,  3.9189e-01,  2.8847e-02,  1.8020e+00,  8.4557e-01,\n",
      "          1.9015e+00],\n",
      "        [-7.2369e-01, -6.9948e-01, -4.8576e-01, -1.8268e-01,  3.0002e-01,\n",
      "          6.4922e-01, -1.0238e+00,  1.2436e+00,  1.9017e+00,  1.6556e+00,\n",
      "         -9.3975e-02, -2.8345e-01, -4.1102e-01,  3.9871e-01,  1.3114e+00,\n",
      "          2.8653e-01],\n",
      "        [-9.8400e-01, -1.3076e+00, -1.1889e+00, -7.6570e-01,  1.4094e-01,\n",
      "         -4.3186e-01,  4.7807e-01,  5.1514e-01,  1.2878e+00,  1.9785e+00,\n",
      "         -3.1343e-01,  4.9259e-02,  1.9563e+00,  3.9665e-01,  3.1259e-01,\n",
      "          2.0176e-01],\n",
      "        [ 7.4974e-01, -1.2281e+00,  1.0667e+00, -5.3197e-02, -1.0944e+00,\n",
      "         -9.0364e-01, -1.1714e+00, -7.2674e-01,  1.6587e+00, -7.5287e-01,\n",
      "         -4.9683e-01,  4.2717e-01,  7.3049e-02, -1.0657e-01,  1.6654e+00,\n",
      "          1.2891e-01]], device='cuda:0')\n",
      "Layer: decoder.weight | Weights: tensor([0.7039, 0.4646, 0.3438, 0.5550, 0.2824, 0.6910, 0.6970, 0.4785, 0.2625,\n",
      "        0.7055, 0.7777, 0.2394, 0.7207, 0.7182, 0.5947, 0.5606],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"Layer: {name} | Weights: {param.data}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47108e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "Layer: decoder.weight | Weights: tensor([ 0.7694,  0.7375,  0.7400,  0.8426,  0.6726,  0.5184,  0.8114, -0.0369,\n",
    "         0.8284,  0.7812,  0.6179,  0.5305,  0.7229,  0.0629,  0.1734,  0.5006]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74ebffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Layer: decoder.weight | Weights: tensor([0.2511, 0.4997, 0.5713, 0.3977, 0.4057, 0.6872, 0.3339, 0.1781, 0.3881,\n",
    "        0.8694, 0.3839, 0.3271, 0.4893, 0.7137, 0.7647, 0.5228],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6560bb73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row sum min: 0.2660941779613495\n",
      "Row sum max: 21.876142501831055\n",
      "Row sum mean: 0.6087040901184082\n"
     ]
    }
   ],
   "source": [
    "row_sums=A_tilde_train.sum(dim=1)\n",
    "print(\"Row sum min:\", row_sums.min().item())\n",
    "print(\"Row sum max:\", row_sums.max().item())\n",
    "print(\"Row sum mean:\", row_sums.mean().item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ccba4220",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_edges_and_sample(A, num_samples=None, test_size=0.1, val_size=0.05, random_state=42):\n",
    "    \"\"\"\n",
    "    Efficiently splits edges and samples non-edges.\n",
    "    \n",
    "    Parameters:\n",
    "    - A: scipy.sparse.coo_matrix (adjacency matrix)\n",
    "    - num_samples: Number of non-edges to sample (adjust based on graph size)\n",
    "    - test_size: Proportion of edges/non-edges for testing\n",
    "    - val_size: Proportion of edges/non-edges for validation\n",
    "    - random_state: Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "    - train_edge_index: Edge list for training\n",
    "    - val_edges, test_edges: Validation and test edge lists\n",
    "    - val_non_edges, test_non_edges: Validation and test non-edges\n",
    "    - train_graph: Sparse matrix representing the training graph\n",
    "    \"\"\"\n",
    "    A_coo = coo_matrix(A)\n",
    "    edges = np.vstack((A_coo.row, A_coo.col)).T  # Extract edges\n",
    "    num_nodes = A.shape[0]\n",
    "    \n",
    "    # Normalize edge representation to avoid both (i, j) and (j, i)\n",
    "    edges = np.array([tuple(sorted((i, j))) for i, j in edges])\n",
    "    existing_edges = set(map(tuple, edges))\n",
    "    \n",
    "    # Sample non-edges\n",
    "    if num_samples is None:\n",
    "        num_samples = int(len(edges) * 0.15)  # Sample 15% of edges as non-edges by default\n",
    "    \n",
    "    np.random.seed(random_state)\n",
    "    non_edges = set()\n",
    "    while len(non_edges) < num_samples:\n",
    "        i = np.random.randint(0, num_nodes)\n",
    "        j = np.random.randint(0, num_nodes)\n",
    "        if i != j:\n",
    "            edge = tuple(sorted((i, j)))\n",
    "            if edge not in existing_edges:\n",
    "                non_edges.add(edge)\n",
    "    \n",
    "    non_edges = np.array(list(non_edges))\n",
    "    \n",
    "    # Split edges into train, validation, and test sets\n",
    "    train_edges, temp_edges = train_test_split(edges, test_size=(test_size + val_size), random_state=random_state)\n",
    "    val_edges, test_edges = train_test_split(temp_edges, test_size=(test_size / (test_size + val_size)), random_state=random_state)\n",
    "    \n",
    "    # Split sampled non-edges into validation and test sets\n",
    "    val_non_edges, test_non_edges = train_test_split(non_edges, test_size=(test_size / (test_size + val_size)), random_state=random_state)\n",
    "    \n",
    "    # Create a training graph (without val/test edges)\n",
    "    train_graph = coo_matrix(\n",
    "        (np.ones(len(train_edges)), (train_edges[:, 0], train_edges[:, 1])),\n",
    "        shape=A.shape\n",
    "    )\n",
    "    train_graph = train_graph + train_graph.T  # Ensure symmetry\n",
    "    \n",
    "    # Convert to PyTorch tensors\n",
    "    train_edge_index = torch.tensor(train_edges.T, dtype=torch.long)\n",
    "    val_edges = torch.tensor(val_edges, dtype=torch.long)\n",
    "    test_edges = torch.tensor(test_edges, dtype=torch.long)\n",
    "    val_non_edges = torch.tensor(val_non_edges, dtype=torch.long)\n",
    "    test_non_edges = torch.tensor(test_non_edges, dtype=torch.long)\n",
    "\n",
    "    return train_edge_index, val_edges, test_edges, val_non_edges, test_non_edges, train_graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "0ed7204d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2., device='cuda:0')\n",
      "tensor([0.7071, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch_sparse\n",
    "from torch_geometric.utils import to_scipy_sparse_matrix\n",
    "from torch_geometric.utils import to_dense_adj\n",
    "\n",
    "# Assume edge_index, num_nodes, and remove_edges_and_sample_optimized are defined\n",
    "# Extract train graph adjacency matrix\n",
    "\n",
    "# Extract train graph adjacency matrix\n",
    "num_nodes = train_edge_index.max().item() + 1\n",
    "\n",
    "train_adj_matrix = to_dense_adj(train_edge_index, max_num_nodes=num_nodes)[0] \n",
    "# Convert to SciPy sparse matrix\n",
    "# Create the adjacency matrix from the edge list (train_edge_index)\n",
    "#train_adj_matrix = to_dense_adj(train_edge_index, max_num_nodes=num_nodes)[0]\n",
    "\n",
    "# Enforce symmetry (add the transpose to ensure both directions are captured)\n",
    "train_adj_matrix = train_adj_matrix + train_adj_matrix.T\n",
    "\n",
    "# Ensure that the diagonal entries are 1 (self-loops)\n",
    "train_adj_matrix.fill_diagonal_(1.0)\n",
    "train_adj_matrix = train_adj_matrix.clamp(max=1)\n",
    "#train_adj_matrix = to_scipy_sparse_matrix(train_edge_index, num_nodes=num_nodes)\n",
    "\n",
    "# Convert directly to a PyTorch sparse tensor\n",
    "#train_adj_matrix = torch.tensor(train_adj_matrix.toarray(), dtype=torch.float32) \n",
    "\n",
    "# Normalize adjacency for training graph\n",
    "A_tilde_train = normalize_adjacency_dense_gpu(train_adj_matrix.to(torch.float32))  # Normalize\n",
    "#D = torch.diag(train_adj_matrix.sum(dim=1).clamp(min=1).pow(-0.5))\n",
    "#A_tilde_train = D @ train_adj_matrix @ D\n",
    "\n",
    "\n",
    "# Initialize model\n",
    "input_dim = X.shape[1]\n",
    "hidden_dim = 32\n",
    "latent_dim = 16\n",
    "model = VGAE_W(input_dim, hidden_dim, latent_dim)\n",
    "\n",
    "model = model.to('cuda')  # Move model to GPU if available\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "88260e82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 106998472.0\n",
      "Epoch 2, Loss: 104572016.0\n",
      "Epoch 3, Loss: 102433448.0\n",
      "Epoch 4, Loss: 100580400.0\n",
      "Epoch 5, Loss: 98991800.0\n",
      "Epoch 6, Loss: 97562920.0\n",
      "Epoch 7, Loss: 96296664.0\n",
      "Epoch 8, Loss: 95181576.0\n",
      "Epoch 9, Loss: 94016464.0\n",
      "Epoch 10, Loss: 93077608.0\n",
      "Epoch 11, Loss: 92143856.0\n",
      "Epoch 12, Loss: 91420328.0\n",
      "Epoch 13, Loss: 90531512.0\n",
      "Epoch 14, Loss: 89728112.0\n",
      "Epoch 15, Loss: 89040912.0\n",
      "Epoch 16, Loss: 88336384.0\n",
      "Epoch 17, Loss: 87677632.0\n",
      "Epoch 18, Loss: 87146328.0\n",
      "Epoch 19, Loss: 86603424.0\n",
      "Epoch 20, Loss: 86117480.0\n",
      "Epoch 21, Loss: 85684856.0\n",
      "Epoch 22, Loss: 85355400.0\n",
      "Epoch 23, Loss: 85010264.0\n",
      "Epoch 24, Loss: 84691120.0\n",
      "Epoch 25, Loss: 84410040.0\n",
      "Epoch 26, Loss: 84201440.0\n",
      "Epoch 27, Loss: 84008528.0\n",
      "Epoch 28, Loss: 83836040.0\n",
      "Epoch 29, Loss: 83664336.0\n",
      "Epoch 30, Loss: 83478576.0\n",
      "Epoch 31, Loss: 83358520.0\n",
      "Epoch 32, Loss: 83277936.0\n",
      "Epoch 33, Loss: 83197328.0\n",
      "Epoch 34, Loss: 83136488.0\n",
      "Epoch 35, Loss: 83016320.0\n",
      "Epoch 36, Loss: 82929000.0\n",
      "Epoch 37, Loss: 82849368.0\n",
      "Epoch 38, Loss: 82761312.0\n",
      "Epoch 39, Loss: 82679184.0\n",
      "Epoch 40, Loss: 82617896.0\n",
      "Epoch 41, Loss: 82538824.0\n",
      "Epoch 42, Loss: 82491880.0\n",
      "Epoch 43, Loss: 82399800.0\n",
      "Epoch 44, Loss: 82354984.0\n",
      "Epoch 45, Loss: 82284032.0\n",
      "Epoch 46, Loss: 82221552.0\n",
      "Epoch 47, Loss: 82161072.0\n",
      "Epoch 48, Loss: 82117856.0\n",
      "Epoch 49, Loss: 82051928.0\n",
      "Epoch 50, Loss: 82005144.0\n",
      "Epoch 51, Loss: 81972824.0\n",
      "Epoch 52, Loss: 81946816.0\n",
      "Epoch 53, Loss: 81921024.0\n",
      "Epoch 54, Loss: 81897408.0\n",
      "Epoch 55, Loss: 81876408.0\n",
      "Epoch 56, Loss: 81859912.0\n",
      "Epoch 57, Loss: 81840968.0\n",
      "Epoch 58, Loss: 81819800.0\n",
      "Epoch 59, Loss: 81807176.0\n",
      "Epoch 60, Loss: 81791064.0\n",
      "Epoch 61, Loss: 81772832.0\n",
      "Epoch 62, Loss: 81753320.0\n",
      "Epoch 63, Loss: 81740088.0\n",
      "Epoch 64, Loss: 81721424.0\n",
      "Epoch 65, Loss: 81704704.0\n",
      "Epoch 66, Loss: 81689672.0\n",
      "Epoch 67, Loss: 81673608.0\n",
      "Epoch 68, Loss: 81660656.0\n",
      "Epoch 69, Loss: 81651208.0\n",
      "Epoch 70, Loss: 81640408.0\n",
      "Epoch 71, Loss: 81632640.0\n",
      "Epoch 72, Loss: 81622152.0\n",
      "Epoch 73, Loss: 81618448.0\n",
      "Epoch 74, Loss: 81605720.0\n",
      "Epoch 75, Loss: 81599608.0\n",
      "Epoch 76, Loss: 81590448.0\n",
      "Epoch 77, Loss: 81584920.0\n",
      "Epoch 78, Loss: 81578872.0\n",
      "Epoch 79, Loss: 81568880.0\n",
      "Epoch 80, Loss: 81555744.0\n",
      "Epoch 81, Loss: 81550016.0\n",
      "Epoch 82, Loss: 81533640.0\n",
      "Epoch 83, Loss: 81525056.0\n",
      "Epoch 84, Loss: 81516432.0\n",
      "Epoch 85, Loss: 81511784.0\n",
      "Epoch 86, Loss: 81503432.0\n",
      "Epoch 87, Loss: 81495584.0\n",
      "Epoch 88, Loss: 81494296.0\n",
      "Epoch 89, Loss: 81488704.0\n",
      "Epoch 90, Loss: 81484696.0\n",
      "Epoch 91, Loss: 81476096.0\n",
      "Epoch 92, Loss: 81473104.0\n",
      "Epoch 93, Loss: 81468536.0\n",
      "Epoch 94, Loss: 81465424.0\n",
      "Epoch 95, Loss: 81462064.0\n",
      "Epoch 96, Loss: 81457176.0\n",
      "Epoch 97, Loss: 81451936.0\n",
      "Epoch 98, Loss: 81449000.0\n",
      "Epoch 99, Loss: 81444888.0\n",
      "Epoch 100, Loss: 81444384.0\n",
      "Epoch 101, Loss: 81439648.0\n",
      "Epoch 102, Loss: 81436760.0\n",
      "Epoch 103, Loss: 81431520.0\n",
      "Epoch 104, Loss: 81429504.0\n",
      "Epoch 105, Loss: 81427312.0\n",
      "Epoch 106, Loss: 81423304.0\n",
      "Epoch 107, Loss: 81419744.0\n",
      "Epoch 108, Loss: 81418504.0\n",
      "Epoch 109, Loss: 81415424.0\n",
      "Epoch 110, Loss: 81412224.0\n",
      "Epoch 111, Loss: 81409344.0\n",
      "Epoch 112, Loss: 81404536.0\n",
      "Epoch 113, Loss: 81402336.0\n",
      "Epoch 114, Loss: 81395536.0\n",
      "Epoch 115, Loss: 81388312.0\n",
      "Epoch 116, Loss: 81387176.0\n",
      "Epoch 117, Loss: 81382976.0\n",
      "Epoch 118, Loss: 81375656.0\n",
      "Epoch 119, Loss: 81371640.0\n",
      "Epoch 120, Loss: 81368552.0\n",
      "Epoch 121, Loss: 81361176.0\n",
      "Epoch 122, Loss: 81360640.0\n",
      "Epoch 123, Loss: 81355344.0\n",
      "Epoch 124, Loss: 81353664.0\n",
      "Epoch 125, Loss: 81349264.0\n",
      "Epoch 126, Loss: 81343840.0\n",
      "Epoch 127, Loss: 81345248.0\n",
      "Epoch 128, Loss: 81338616.0\n",
      "Epoch 129, Loss: 81335232.0\n",
      "Epoch 130, Loss: 81335456.0\n",
      "Epoch 131, Loss: 81328624.0\n",
      "Epoch 132, Loss: 81324888.0\n",
      "Epoch 133, Loss: 81324904.0\n",
      "Epoch 134, Loss: 81324176.0\n",
      "Epoch 135, Loss: 81322136.0\n",
      "Epoch 136, Loss: 81315896.0\n",
      "Epoch 137, Loss: 81315816.0\n",
      "Epoch 138, Loss: 81308656.0\n",
      "Epoch 139, Loss: 81307728.0\n",
      "Epoch 140, Loss: 81306032.0\n",
      "Epoch 141, Loss: 81302688.0\n",
      "Epoch 142, Loss: 81301848.0\n",
      "Epoch 143, Loss: 81297992.0\n",
      "Epoch 144, Loss: 81295040.0\n",
      "Epoch 145, Loss: 81287880.0\n",
      "Epoch 146, Loss: 81289648.0\n",
      "Epoch 147, Loss: 81288008.0\n",
      "Epoch 148, Loss: 81284512.0\n",
      "Epoch 149, Loss: 81283672.0\n",
      "Epoch 150, Loss: 81279048.0\n",
      "Epoch 151, Loss: 81277192.0\n",
      "Epoch 152, Loss: 81272800.0\n",
      "Epoch 153, Loss: 81271928.0\n",
      "Epoch 154, Loss: 81267984.0\n",
      "Epoch 155, Loss: 81270096.0\n",
      "Epoch 156, Loss: 81263584.0\n",
      "Epoch 157, Loss: 81261632.0\n",
      "Epoch 158, Loss: 81257960.0\n",
      "Epoch 159, Loss: 81258000.0\n",
      "Epoch 160, Loss: 81253352.0\n",
      "Epoch 161, Loss: 81253432.0\n",
      "Epoch 162, Loss: 81247888.0\n",
      "Epoch 163, Loss: 81246904.0\n",
      "Epoch 164, Loss: 81245016.0\n",
      "Epoch 165, Loss: 81238416.0\n",
      "Epoch 166, Loss: 81241384.0\n",
      "Epoch 167, Loss: 81241280.0\n",
      "Epoch 168, Loss: 81235464.0\n",
      "Epoch 169, Loss: 81238184.0\n",
      "Epoch 170, Loss: 81231656.0\n",
      "Epoch 171, Loss: 81228776.0\n",
      "Epoch 172, Loss: 81224904.0\n",
      "Epoch 173, Loss: 81225096.0\n",
      "Epoch 174, Loss: 81220528.0\n",
      "Epoch 175, Loss: 81220872.0\n",
      "Epoch 176, Loss: 81215112.0\n",
      "Epoch 177, Loss: 81215144.0\n",
      "Epoch 178, Loss: 81216776.0\n",
      "Epoch 179, Loss: 81212096.0\n",
      "Epoch 180, Loss: 81207072.0\n",
      "Epoch 181, Loss: 81208664.0\n",
      "Epoch 182, Loss: 81205744.0\n",
      "Epoch 183, Loss: 81202440.0\n",
      "Epoch 184, Loss: 81198976.0\n",
      "Epoch 185, Loss: 81203072.0\n",
      "Epoch 186, Loss: 81199320.0\n",
      "Epoch 187, Loss: 81192440.0\n",
      "Epoch 188, Loss: 81192872.0\n",
      "Epoch 189, Loss: 81189640.0\n",
      "Epoch 190, Loss: 81188624.0\n",
      "Epoch 191, Loss: 81185320.0\n",
      "Epoch 192, Loss: 81182808.0\n",
      "Epoch 193, Loss: 81185976.0\n",
      "Epoch 194, Loss: 81179440.0\n",
      "Epoch 195, Loss: 81178584.0\n",
      "Epoch 196, Loss: 81176136.0\n",
      "Epoch 197, Loss: 81175336.0\n",
      "Epoch 198, Loss: 81172744.0\n",
      "Epoch 199, Loss: 81169368.0\n",
      "Epoch 200, Loss: 81165792.0\n"
     ]
    }
   ],
   "source": [
    "# Training OLD\n",
    "num_epochs = 400\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass\n",
    "    Z, A_reconstructed, mu, log_sigma = model(X.to('cuda'), A_tilde_train.to('cuda'))\n",
    "    \n",
    "    # Clamp log_sigma to prevent extreme values\n",
    "    log_sigma = torch.clamp(log_sigma, min=-18, max=18)\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = loss_function(train_adj_matrix.to('cuda'), A_reconstructed.to('cuda'), mu, log_sigma)\n",
    "    loss.backward()\n",
    "\n",
    "    # Update parameters\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "c9ee4f7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch-local/scur2863.9735026/ipykernel_999491/3313984776.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_edges = torch.tensor(test_edges, dtype=torch.long)\n",
      "/scratch-local/scur2863.9735026/ipykernel_999491/3313984776.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_non_edges = torch.tensor(test_non_edges, dtype=torch.long)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Convert the reconstructed adjacency matrix to CPU if necessary\n",
    "A_reconstructed = A_reconstructed.detach().cpu()\n",
    "\n",
    "# Ensure test edges and non-edges are tensors\n",
    "test_edges = torch.tensor(test_edges, dtype=torch.long)\n",
    "test_non_edges = torch.tensor(test_non_edges, dtype=torch.long)\n",
    "\n",
    "# Handle different decoder outputs\n",
    "if A_reconstructed.dim() == 2:  \n",
    "    # If A_reconstructed is a full adjacency matrix\n",
    "    test_edge_scores = A_reconstructed[test_edges[:, 0], test_edges[:, 1]].numpy()\n",
    "    test_non_edge_scores = A_reconstructed[test_non_edges[:, 0], test_non_edges[:, 1]].numpy()\n",
    "else:\n",
    "    # If A_reconstructed is a 1D tensor (edge probabilities only)\n",
    "    test_edge_scores = A_reconstructed[:len(test_edges)].numpy()\n",
    "    test_non_edge_scores = A_reconstructed[len(test_edges):].numpy()\n",
    "\n",
    "# Combine scores and create labels\n",
    "scores = np.concatenate([test_edge_scores, test_non_edge_scores])\n",
    "labels = np.concatenate([np.ones(len(test_edge_scores)), np.zeros(len(test_non_edge_scores))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f569b3c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC Score: 0.6941392138324446\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Assuming y_true contains actual edge labels (1 for edges, 0 for non-edges)\n",
    "# and y_score contains the predicted scores for each pair of nodes\n",
    "roc_auc = roc_auc_score(labels, scores)\n",
    "print(f\"ROC-AUC Score: {roc_auc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "8aa7448d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision (AP): 0.7416\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import average_precision_score\n",
    "ap_score = average_precision_score(labels, scores)\n",
    "print(f\"Average Precision (AP): {ap_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a18cc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl 0.4957 y 0.11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d3d434",
   "metadata": {},
   "outputs": [],
   "source": [
    "inner product 0.5692 y 0.5341"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a037bc24",
   "metadata": {},
   "source": [
    "## GAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cb174de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GCNLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Graph Convolutional Layer for GAE\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(GCNLayer, self).__init__()\n",
    "        self.weight = nn.Parameter(torch.randn(in_channels, out_channels))\n",
    "        self.bias = nn.Parameter(torch.zeros(out_channels))\n",
    "\n",
    "    def forward(self, X, A):\n",
    "        \"\"\"\n",
    "        X: Node features matrix (num_nodes, input_dim)\n",
    "        A: Adjacency matrix (num_nodes, num_nodes)\n",
    "        \"\"\"\n",
    "        support = torch.matmul(X, self.weight)  # Apply linear transformation\n",
    "        output = torch.matmul(A, support)  # Aggregate neighbor information\n",
    "        return output + self.bias  # Add bias term\n",
    "\n",
    "class GAE(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
    "        super(GAE, self).__init__()\n",
    "        # Encoder layers\n",
    "        self.gcn1 = GCNLayer(input_dim, hidden_dim)\n",
    "        self.gcn2 = GCNLayer(hidden_dim, latent_dim)\n",
    "\n",
    "    def forward(self, X, A_tilde):\n",
    "        \"\"\"\n",
    "        X: Node feature matrix (num_nodes, input_dim)\n",
    "        A_tilde: Normalized adjacency matrix (num_nodes, num_nodes)\n",
    "        \"\"\"\n",
    "        # Graph convolution layer 1\n",
    "        H = F.relu(self.gcn1(X, A_tilde))  # First GCN layer with ReLU activation\n",
    "        \n",
    "        # Graph convolution layer 2\n",
    "        Z = self.gcn2(H, A_tilde)  # Second GCN layer for embeddings Z\n",
    "        \n",
    "        # Normalize the embeddings (optional, based on your specific use case)\n",
    "        Z = F.normalize(Z, p=2, dim=1)  # Normalize each row of Z to have unit length\n",
    "        \n",
    "        # Reconstruct the adjacency matrix\n",
    "        A_reconstructed = torch.sigmoid(torch.matmul(Z, Z.T))  # Reconstruct the adjacency matrix\n",
    "        \n",
    "        return Z, A_reconstructed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "40f66709",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(A, A_reconstructed):\n",
    "    \"\"\"\n",
    "    A: True adjacency matrix (num_nodes, num_nodes)\n",
    "    A_reconstructed: Reconstructed adjacency matrix (num_nodes, num_nodes)\n",
    "    \"\"\"\n",
    "    # Flatten both matrices to make them compatible for loss computation\n",
    "    A = A.view(-1)\n",
    "    A_reconstructed = A_reconstructed.view(-1)\n",
    "    \n",
    "    # Compute binary cross-entropy loss\n",
    "    return F.binary_cross_entropy(A_reconstructed, A)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5eb6b9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.utils import to_dense_adj\n",
    "\n",
    "# Assume edge_index, num_nodes, and remove_edges_and_sample_optimized are defined\n",
    "# Extract train graph adjacency matrix\n",
    "train_adj_matrix = to_dense_adj(train_edge_index, max_num_nodes=num_nodes)[0]  # Convert to dense adjacency matrix\n",
    "train_adj_matrix = train_adj_matrix.to(torch.float32)  # Ensure float type for computations\n",
    "\n",
    "# Node features\n",
    "#if data.x is not None:\n",
    " #   X = data.x  # Use provided node features\n",
    "#else:\n",
    "X = torch.eye(num_nodes)  # Use identity matrix if featureless\n",
    "\n",
    "# Normalize adjacency for training graph\n",
    "A_tilde = normalize_adjacency_dense_gpu(train_adj_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "89c333b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/200, Loss: 0.7251855731010437\n",
      "Epoch 20/200, Loss: 0.7126666307449341\n",
      "Epoch 30/200, Loss: 0.709176778793335\n",
      "Epoch 40/200, Loss: 0.7057015299797058\n",
      "Epoch 50/200, Loss: 0.7053874731063843\n",
      "Epoch 60/200, Loss: 0.7044334411621094\n",
      "Epoch 70/200, Loss: 0.7040550112724304\n",
      "Epoch 80/200, Loss: 0.7036513090133667\n",
      "Epoch 90/200, Loss: 0.7033585906028748\n",
      "Epoch 100/200, Loss: 0.7030957937240601\n",
      "Epoch 110/200, Loss: 0.7028746604919434\n",
      "Epoch 120/200, Loss: 0.702678382396698\n",
      "Epoch 130/200, Loss: 0.7025027871131897\n",
      "Epoch 140/200, Loss: 0.7023438811302185\n",
      "Epoch 150/200, Loss: 0.7021982073783875\n",
      "Epoch 160/200, Loss: 0.702064037322998\n",
      "Epoch 170/200, Loss: 0.7019408345222473\n",
      "Epoch 180/200, Loss: 0.7018279433250427\n",
      "Epoch 190/200, Loss: 0.7017245888710022\n",
      "Epoch 200/200, Loss: 0.7016303539276123\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "input_dim = X.shape[1]  # Number of features per node\n",
    "hidden_dim = 32  # Hidden layer size\n",
    "latent_dim = 16  # Latent space size (embedding dimension)\n",
    "num_epochs = 200\n",
    "learning_rate = 0.01\n",
    "device='cuda'\n",
    "# Initialize model and optimizer\n",
    "model = GAE(input_dim, hidden_dim, latent_dim).to(device) \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    Z, A_reconstructed = model(X.to(device), train_adj_matrix.to(device))\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = loss_function(train_adj_matrix.to(device), A_reconstructed)\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update parameters\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "20ec5f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC Score: 0.8473404554648748\n",
      "Average Precision (AP): 0.8702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch-local/scur2863.9595676/ipykernel_1494540/598414332.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_edges = torch.tensor(test_edges, dtype=torch.long)\n",
      "/scratch-local/scur2863.9595676/ipykernel_1494540/598414332.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_non_edges = torch.tensor(test_non_edges, dtype=torch.long)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "\n",
    "\n",
    "# Convert the reconstructed adjacency matrix to CPU if necessary\n",
    "A_reconstructed = A_reconstructed.detach().cpu()\n",
    "\n",
    "test_edges = torch.tensor(test_edges, dtype=torch.long)\n",
    "test_non_edges = torch.tensor(test_non_edges, dtype=torch.long)\n",
    "# Get the scores for test edges and test non-edges\n",
    "test_edge_scores = A_reconstructed[test_edges[:, 0], test_edges[:, 1]].numpy()\n",
    "test_non_edge_scores = A_reconstructed[test_non_edges[:, 0], test_non_edges[:, 1]].numpy()\n",
    "\n",
    "# Combine scores and create labels\n",
    "scores = np.concatenate([test_edge_scores, test_non_edge_scores])\n",
    "labels = np.concatenate([np.ones(len(test_edge_scores)), np.zeros(len(test_non_edge_scores))])\n",
    "\n",
    "# Assuming y_true contains actual edge labels (1 for edges, 0 for non-edges)\n",
    "# and y_score contains the predicted scores for each pair of nodes\n",
    "roc_auc = roc_auc_score(labels, scores)\n",
    "print(f\"ROC-AUC Score: {roc_auc}\")\n",
    "\n",
    "ap_score = average_precision_score(labels, scores)\n",
    "print(f\"Average Precision (AP): {ap_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9363e746",
   "metadata": {},
   "source": [
    "#### Appendix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1c5b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "def deepwalk(graph, num_walks=10, walk_length=80, window_size=10):\n",
    "    walks = []\n",
    "    for node in graph.nodes():\n",
    "        for _ in range(num_walks):\n",
    "            walk = [node]\n",
    "            for _ in range(walk_length):\n",
    "                neighbors = list(graph.neighbors(walk[-1]))\n",
    "                walk.append(np.random.choice(neighbors))\n",
    "            walks.append(walk)\n",
    "    model = Word2Vec(walks, vector_size=128, window=window_size, min_count=1, sg=1)\n",
    "    return model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e75e06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import SpectralClustering\n",
    "\n",
    "sc = SpectralClustering(n_clusters=2, affinity='precomputed')\n",
    "sc_embeddings = sc.fit_predict(adjacency_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b793a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Convert the reconstructed adjacency matrix to CPU if necessary\n",
    "A_reconstructed = A_reconstructed.detach().cpu()\n",
    "\n",
    "test_edges = torch.tensor(test_edges, dtype=torch.long)\n",
    "test_non_edges = torch.tensor(test_non_edges, dtype=torch.long)\n",
    "# Get the scores for test edges and test non-edges\n",
    "test_edge_scores = A_reconstructed[test_edges[:, 0], test_edges[:, 1]].numpy()\n",
    "test_non_edge_scores = A_reconstructed[test_non_edges[:, 0], test_non_edges[:, 1]].numpy()\n",
    "\n",
    "# Combine scores and create labels\n",
    "scores = np.concatenate([test_edge_scores, test_non_edge_scores])\n",
    "labels = np.concatenate([np.ones(len(test_edge_scores)), np.zeros(len(test_non_edge_scores))])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
